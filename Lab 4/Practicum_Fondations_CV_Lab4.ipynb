{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"id":"lO7irFwVI3rO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702655556269,"user_tz":-60,"elapsed":2449,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"}},"outputId":"0866de3f-cc8b-41d3-8e90-f9e3d9a5e16a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Importing working libraries\n","import os\n","import cv2\n","import urllib3\n","import imutils\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","# Select your development environment\n","environment = \"Google Colab\" # \"Google Colab\", \"Local\"\n","\n","# Loading depending on environment\n","lab4_path = \"\"\n","if environment == \"Google Colab\":\n","  from google.colab import drive\n","  from google.colab.patches import cv2_imshow\n","  # Import from my google drive\n","  drive.mount('/content/drive')\n","  lab4_path = \"/content/drive/MyDrive/Colab Notebooks/UAB Project/Computer Vision/UAB23-Computer-Vision-Labs/Lab 4\"\n","else:\n","  lab4_path = \"./\"\n","\n","# Load images path\n","path_images = os.path.join(lab4_path,\"dataset\")\n","path_images_train = os.path.join(path_images,\"train\")\n","path_images_test = os.path.join(path_images,\"test\")"]},{"cell_type":"markdown","metadata":{"id":"jYfBhcLYhLrw"},"source":["# Block 4. Feature detection, Image descriptors and BoVW"]},{"cell_type":"markdown","metadata":{"id":"_exgHAGtQWrF"},"source":["This lab explores the fundamental concepts of feature extraction and feature descriptors in computer vision. It delves into various feature detection techniques like Harris corner and LoG blob detectors, and descriptor methods like SIFT, SURF, and ORB. Additionally, the lab introduces the Bag of Visual Words model, demonstrating its application in image classification, particularly using a subset of the VOC dataset.\n","\n","### Objectives:\n","1. **Implement Feature extraction methods**\n","  - **Implement Harris Corner Detector**: Understand and implement the Harris corner detection algorithm to identify corner features in images.\n","  - **Implement LoG Blob Detector**: Learn and apply the Laplacian of Gaussian method for blob detection in images.\n","2. **Implement Image feature descriptors** (Reusing part of the feature extraction code)\n","  - **Implement Normalized Color Histogram and Color Space Changes**: Explore color features by implementing normalized color histograms and understanding the impact of different color spaces.\n","  - **Use SIFT, SURF, and ORB**: Use these feature descriptors, analyze their strengths and weaknesses, and compare their performance.\n","3. **Provide a Pipeline for Bag of Words and Compute Results on Subset of VOC Dataset**: Develop a Bag of Visual Words model and test its efficacy in classifying images from a subset of the VOC dataset.\n","\n","---\n","\n","### Mandatory Questions:\n","\n","1. Explain the principles behind corner detectors, specifically the Harris corner detection method. Why is it effective in identifying corners?\n","2. Discuss the main differences between edge detection and corner detection. Why are corners considered more robust features?\n","3. What are the limitations of the Harris corner detector in complex images?\n","4. Describe the Laplacian of Gaussian blob detector. How does it differ from edge detection?\n","5. Discuss the role of feature descriptors in object recognition.\n","6. Why are color histograms useful for image feature extraction? Describe the process of creating a normalized color histogram.\n","7. How does a change in color space affect feature extraction in images?\n","8. Explain the concepts of scale and rotation invariance in feature detection. For each algorithm (SIFT, SURF, and ORB), explain how they handle scale and rotation variations in images.\n","9. Compare SIFT, SURF, and ORB in terms of computational efficiency and accuracy.\n","10. What is the role of orientation assignment in the SIFT algorithm?\n","11. How does the ORB algorithm differ from SIFT and SURF in terms of feature matching?\n","12. How do feature descriptors contribute to the process of image stitching (for panorama images)?\n","13. Discuss the impact of varying lighting conditions on feature detection and description. How can these challenges be mitigated?\n","14. What are the key steps in constructing a Bag of Visual Words model? Explain each of them in detail.\n","15. In the context of the Bag of Visual Words model, explain the significance of feature quantization. Discuss on which steps it can be done and how it affects the results.\n","\n","\n","### Optional Deep Dive Questions:\n","\n","1. Discuss the potential weaknesses of the LoG blob detector in real-world scenarios. Propose improvements.\n","2. Explore the potential of hybrid color spaces (combining properties of different color models) in improving the robustness of feature extraction for applications in varied lighting and environmental conditions.\n","3. Discuss the impact of varying image resolutions on the effectiveness of feature descriptors.\n","4. Discuss the limitations of current feature descriptors in handling occlusions and propose potential improvements.\n","5. Evaluate the performance of hybrid feature detectors that combine the properties of multiple algorithms (like SIFT and Harris). What are the potential benefits?\n","6. Analyze the role of feature extraction in content-based image retrieval systems. How do they impact the efficiency and accuracy of searches?\n","7. How might the Bag of Visual Words model be modified or extended to improve classification accuracy?\n","8. Consider the role of context in feature-based image classification. How might contextual information be incorporated into the Bag of Visual Words model?\n"]},{"cell_type":"markdown","metadata":{"id":"bbahKlO7avjP"},"source":["### Exercise 1: Harris Corner Detector Implementation\n","**Objective**:\n","\n","Implement the Harris Corner Detection algorithm to identify corner features in images.\n","\n","**Guideline**:\n","- Load image.\n","- Implement the basic Harris Corner Detection algorithm.\n","- Apply the algorithm to the images and visualize the detected corners.\n","- Explore the impact of parameter changes on detection quality.\n","- (Optional) Implement the Harris-Laplace and the Harris-Affine improvments and compare the pros and cons of each version\n","\n","**Expected Results**:\n","- A working Harris Corner Detection implementation.\n","- Visualization of corner detection in various images."]},{"cell_type":"code","source":["def harris_corner_detector(image, k=0.04, threshold=0.01):\n","    # Convert the image to grayscale\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).astype(np.float32)\n","\n","    # Compute gradients using Sobel operators\n","    Ix = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n","    Iy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n","\n","    # Compute elements of the Harris matrix\n","    Ixx = Ix * Ix\n","    Iyy = Iy * Iy\n","    Ixy = Ix * Iy\n","\n","    # Compute the sum of squared differences\n","    Sxx = cv2.GaussianBlur(Ixx, (3, 3), 0)\n","    Syy = cv2.GaussianBlur(Iyy, (3, 3), 0)\n","    Sxy = cv2.GaussianBlur(Ixy, (3, 3), 0)\n","\n","    # Compute the determinant and trace of the Harris matrix\n","    det = (Sxx * Syy) - (Sxy ** 2)\n","    trace = Sxx + Syy\n","\n","    # Compute the Harris response\n","    harris_response = det - k * (trace ** 2)\n","\n","    # Threshold the response to identify corners\n","    corners = np.where(harris_response > threshold * harris_response.max())\n","\n","    # Draw circles around the detected corners\n","    result = image.copy()\n","    result[corners] = [0, 0, 255]  # Red color for corners\n","\n","    return result\n","\n","# List of images\n","images_path = [\n","    os.path.join(path_images_train,\"aquarium\",\"sun_acjrhiyqadrbtovi.jpg\"),\n","    os.path.join(path_images_train,\"aquarium\",\"sun_aamlnkpomzzujqpa.jpg\"),\n","    os.path.join(path_images_train,\"aquarium\",\"sun_aashbnpnxflmstxu.jpg\"),\n","    os.path.join(path_images_train,\"kitchen\",\"sun_afbibxsnpvkmvhxn.jpg\"),\n","    os.path.join(path_images_train,\"kitchen\",\"sun_acgxyyrqtufcunpm.jpg\"),\n","    os.path.join(path_images_train,\"kitchen\",\"sun_ahyafiyoyrzkifbh.jpg\")\n","]\n","\n","# Parameters variations\n","k_values = [0.02, 0.04, 0.06]\n","threshold_values = [0.005, 0.01, 0.015]\n","\n","for image_path in images_path:\n","    # Load the image\n","    original_image = cv2.imread(image_path)\n","\n","    # Display the results using Matplotlib for each parameter variation\n","    for k in k_values:\n","        for threshold in threshold_values:\n","            # Apply Harris Corner Detector (Your Implementation)\n","            result_image_your_impl = harris_corner_detector(original_image, k, threshold)\n","\n","            # Apply Harris Corner Detector (OpenCV Implementation)\n","            gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n","            gray = np.float32(gray)\n","            harris_response_cv = cv2.cornerHarris(gray, 2, 3, k)\n","\n","            # Threshold the response to identify corners in OpenCV's implementation\n","            corners_cv = np.where(harris_response_cv > threshold * harris_response_cv.max())\n","\n","            # Draw circles around the detected corners in OpenCV's implementation\n","            result_image_opencv_impl = original_image.copy()\n","            result_image_opencv_impl[corners_cv[0], corners_cv[1]] = [0, 255, 0]  # Correct way to mark corners\n","\n","            # Display the results using Matplotlib\n","            plt.figure(figsize=(15, 6))\n","\n","            plt.subplot(231)\n","            plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n","            plt.title('Original Image')\n","            plt.axis('off')\n","\n","            plt.subplot(232)\n","            plt.imshow(cv2.cvtColor(result_image_your_impl, cv2.COLOR_BGR2RGB))\n","            plt.title(f'Our Harris (k={k}, th={threshold})')\n","            plt.axis('off')\n","\n","            plt.subplot(233)\n","            plt.imshow(cv2.cvtColor(result_image_opencv_impl, cv2.COLOR_BGR2RGB))\n","            plt.title(f'OpenCV Harris (k={k}, th={threshold})')\n","            plt.axis('off')\n","\n","            plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1yXVbNh2bE4cqJxFeq4jR_ZyDlVbYaD5Z"},"id":"hO7T9PwjJh27","executionInfo":{"status":"ok","timestamp":1702656135459,"user_tz":-60,"elapsed":25048,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"}},"outputId":"e9581f78-b9f4-4ff0-fcf2-b79f7712afa3"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"bK5w-3kW54Lo"},"source":["\n","### Exercise 2: LoG Blob Detector Implementation\n","**Objective**:\n","\n","Develop and apply the LoG method for blob detection in images, understanding its application in feature extraction.\n","\n","**Guideline**:\n","- Select diverse images for blob detection.\n","- Implement the LoG Blob Detector.\n","- Analyze the LoG method's effectiveness.\n","- Compare LoG with other blob detection methods.\n","- (Optional) Implement your own version of other methods (DoG, DoH, ...) or design your own\n","\n","**Expected Results**:\n","- Documented LoG Blob Detector implementation.\n","- Comparative analysis of blob detection results.\n","- Evaluation of method effectiveness and limitations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzB8ZGlPBv3W"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Sb0Hp7LiExhs"},"source":["\n","### Exercise 3: Normalized Color Histogram and Color Space Changes\n","**Objective**:\n","Implement normalized color histograms and explore color space transformations' impact on feature extraction.\n","\n","**Guideline**:\n","- Implement normalized color histograms.\n","- Transform images into different color spaces and generate histograms.\n","- Analyze the impact of color space changes.\n","\n","**Expected Results**:\n","- Color histograms in various spaces for a set of images.\n","- Comparative analysis on color space impact.\n","- Discussion on color in feature extraction and analysis.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pjrs6L68L8kl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Icgv4u4aLIqb"},"source":["### Exercise 4: Implementation and Analysis of SIFT, SURF, and ORB\n","**Objective**:\n","Implement and analyze SIFT, SURF, and ORB feature descriptors (use opencv implementations), understanding their pros and cons.\n","\n","**Guideline**:\n","- Select a diverse image dataset.\n","- Implement SIFT and ORB algorithms. (Since SURF is patented, is optionally in the comparions)\n","- Apply these methods to some images.\n","\n","**Expected Results**:\n","- Implementations of SIFT, ORB (and SURF).\n","- Hyperparameter analysis of each algorithm.\n","- Comparision of the methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tR6bNTRPBv3a"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DG_AL9sc33Vi"},"source":["### (Optional) Exercise 4.5: Implement other image descriptor methods (motion, shape, ...)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8cv5VOu4EkZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"WCXuuRl7LKyh"},"source":["### Exercise 5: Bag of Visual Words Pipeline and VOC Dataset Analysis\n","**Objective**:\n","Develop a BoVW model and apply it to classify images from the VOC dataset, focusing on the BoVW approach.\n","\n","**Guideline**:\n","- Load the VOC dataset.\n","- Implement the BoVW model.\n","- Apply the model to the dataset for classification.\n","- Evaluate the model's performance.\n","\n","**Expected Results**:\n","- Implementation of the Bag of Visual Words model.\n","- Application of the model for image classification.\n","- Evaluation of the model's performance.\n","\n","**Optional Enhancements**:\n","- Experiment with different clustering algorithms for codebook generation in the BoVW model."]},{"cell_type":"markdown","metadata":{"id":"7_Zag5JjBv3c"},"source":["### Step 1: Data Loading\n","- **Objective**: Load images from each class into a format suitable for processing.\n","- **Guide**: Use a library like OpenCV or PIL in Python to load images. Iterate over the folders, loading each image and storing it with its class label.\n","\n","### Step 2: Feature Extraction\n","- **Objective**: Extract features from each image to represent its content.\n","- **Guide**: Apply a feature descriptor to each image. This transforms the image into a set of features. Examples can be SIFT, SURF, Color histogram, etc\n","\n","### Step 3: Building the Vocabulary (Codebook)\n","- **Objective**: Create a vocabulary of visual words (features) that represents the dataset.\n","- **Guide**: Cluster the extracted features across all images using any clustering algorithm. Each cluster center is a visual word.\n","\n","### Step 4: Feature Encoding\n","- **Objective**: Encode each image as a frequency histogram of visual words.\n","- **Guide**: For each image, count how many features fall into each cluster (visual word) and create a histogram.\n","\n","### Step 5: Classifier Training\n","- **Objective**: Train a machine learning model to classify the images.\n","- **Guide**: Use the histograms as input features for a classifier. Any supervised classifier can be used (SVM, Random Forest, KNN, Neural nets, ...)\n","\n","### Step 6: Model Evaluation\n","- **Objective**: Evaluate the performance of the classifier.\n","- **Guide**: Use a confusion matrix to visualize the performance and extract metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukzewt1gBv3d"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}