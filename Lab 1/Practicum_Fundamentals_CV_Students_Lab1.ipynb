{"cells":[{"cell_type":"markdown","metadata":{"id":"WQAep6mya0Uv"},"source":["# Fundamentals of Computer Vision: Applied Projects\n","\n","\n","This Jupyter notebook series contains the bases of the laboratories you need to develop during the semester.\n","\n","The main goal of those projects is twofold: 1st is to reiterate the theoretical context of the lectures and 2nd to use those methods in adapted examples for real-world scenarios.\n","\n","## Deadlines\n","\n","- **Follow-up:** 26/10 15:00h\n","  - Jupyter executed with all the work done until the moment\n","- **Deliverable** 3/11 23:55h\n","  - Report\n","  - Code\n","  - Data\n","\n","*See practicum presentation slides for more detailed information*"]},{"cell_type":"markdown","metadata":{"id":"xjN6nIR2sdec"},"source":["## Import libraries, util functions and test image loading\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Rhvvwu-HIpI3"},"source":["Libraries that you can use (not mandatory) and some auxiliar functions that could be useful on this block."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21815,"status":"ok","timestamp":1698767316839,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"lO7irFwVI3rO","outputId":"42caf57d-cda0-4f30-cf9d-adfce9dee396"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Importing working libraries\n","import os\n","import cv2\n","import torch\n","import random\n","import urllib3\n","import imutils\n","import math as m\n","import numpy as np\n","import torch.nn as nn\n","import tensorflow as tf\n","from google.colab import drive # Comment when working in VSC\n","from PIL import Image, ImageDraw\n","from scipy.signal import convolve2d\n","from matplotlib import pyplot as plt\n","from google.colab.patches import cv2_imshow # Comment when working in VSC\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","\n","# Import from my google drive\n","drive.mount('/content/drive') # Comment when working in VSC\n","\n","# Load image path to test functions\n","# For VSC:\n","#path_images = \"./images\" # It doesn't work in google colab because the file management is different.\n","# For Google Colab:\n","path_images = \"/content/drive/MyDrive/Colab Notebooks/Computer Vision/UAB23-Computer-Vision-Labs/Lab 1/images\"\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698767081739,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"7TkYSsHcivVp"},"outputs":[],"source":["def add_noise(image, noise_type, mean=0, std=0.01, amount=0.05, salt=0.5):\n","    \"\"\"\n","    Add noise to an image.\n","\n","    Parameters:\n","    - image: numpy array of shape (H, W) for grayscale or (H, W, 3) for RGB\n","    - noise_type: string, one of 'gaussian', 'salt_pepper', 'speckle'\n","    - params: dictionary containing parameters specific to the noise type\n","\n","    Returns:\n","    - noisy_image: numpy array with added noise\n","    \"\"\"\n","\n","    if noise_type == 'gaussian':\n","        sigma = std**0.5\n","        noise = np.random.normal(mean, sigma, image.shape).astype(image.dtype)\n","        noisy_image = cv2.add(image, noise)\n","\n","    elif noise_type == 'salt_pepper':\n","        out = np.copy(image)\n","        # Salt mode\n","        num_salt = np.ceil(amount * image.size * salt)\n","        coords = [np.random.randint(0, i-1, int(num_salt)) for i in image.shape]\n","        out[tuple(coords)] = 1\n","        # Pepper mode\n","        num_pepper = np.ceil(amount*image.size*(1. - salt))\n","        coords = [np.random.randint(0, i-1, int(num_pepper)) for i in image.shape]\n","        out[tuple(coords)] = 0\n","        noisy_image = out\n","\n","    elif noise_type == 'speckle':\n","        noise = np.random.randn(*image.shape).astype(image.dtype)\n","        noisy_image = image + image * noise\n","\n","    else:\n","        raise ValueError(f\"Noise type '{noise_type}' is not supported.\")\n","\n","    # Clip values to be in [0, 255]\n","    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n","\n","    return noisy_image\n","\n","def add_periodic_noise(image, frequency=5, amplitude=10):\n","    X, Y = np.meshgrid(np.arange(image.shape[0]),np.arange(image.shape[1]))\n","\n","    noise = amplitude * np.sin(2 * np.pi * frequency * Y / image.shape[0])\n","\n","    noisy_image = image + noise\n","\n","    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n","\n","    return noisy_image\n"]},{"cell_type":"markdown","metadata":{"id":"DP0E7aMo5yeg"},"source":["# Block 1. Linear filtering\n","This block focuses on linear filtering, one of the foundational concepts in image processing and computer vision. We'll be delving deep into how convolution operates, the distinction between convolution and correlation, and the magic of the Fourier transform in image filtering.\n","\n","### Objectives:\n","\n","1. **Implement and Learn How Convolution Works:**  \n","   A self-implemented convolution function and its application on sample images.\n","\n","2. **Use Convolution to Apply Linear Filters:**  \n","   Blurred and edge-detected versions of input images using your convolution function.\n","\n","3. **Compare Convolution and Correlation:**  \n","   Implementation of correlation function and comparison with convolution.\n","\n","4. **Normalized Cross-Correlation and Use for Template Matching:**  \n","   Function to locate a template within a larger image.\n","\n","5. **Fourier Transform and Image Filtering:**  \n","   Filtered images emphasizing or de-emphasizing certain frequency components.\n","\n","---\n","\n","### Mandatory Questions:\n","\n","- What is the purpose of image filtering in computer vision? Can you list some common applications?\n","   - Attenuate or emphasize features, regions or properties of the scene contained in an image in order to meet a specific need. Image editing, blur, contour detection, noise reduction, smoothing, contrast enhancement, etc.\n","\n","- Why do we often use odd-sized kernels in image filtering? When we want to use even-sized filters?\n","   - Because we usually want to treat each pixel according to the region around it, by using odd-dimensional kernels, we ensure that the kernel will have a single central element that we can use as the center when performing calculations. An even-dimensional kernel would be useful if we want to treat a set of pixels or a region of the image at the same time, also when we want to reduce the computational work and increase the stride in order to process the given image faster.\n","\n","- Why do we often use a flipped version of the kernel in the convolution operation?\n","   - The process lies in the mathematical foundations of convolution, which is a special case of cross-correlation. By flipping the kernel we ensure that the operations correspond correctly to the pixel-by-pixel alignment of the image matrix.\n","\n","- What is the fundamental difference between convolution and correlation? Can you demonstrate this with an example?\n","   - In convolution, kernel flipping is performed, a process that is not performed in correlation, for example, by having the kernel as a basis:\n","      - 1 0 1\n","      - 0 0 1\n","      - 1 0 0\n","   - in the convolution its flipped version would be used:\n","      - 0 0 1\n","      - 1 0 0\n","      - 1 0 1\n","   - while the correlation would keep the original kernel without applying any additional process, obtaining the kernel:\n","      - 1 0 1\n","      - 0 0 1\n","      - 1 0 0\n","\n","- How does changing the size of the kernel affect the outcome of the convolution operation?\n","   - Each pixel will be transformed according to the area surrounding it being this increasingly larger or smaller as the kernel size varies, i.e., using a kernel of dimension 1x1, the pixel will be transformed based only on its properties, by increasing the kernel to 3x3, now the pixel will be transformed not only according to it, but also with respect to its immediate neighbors, in the dimension 5x5 neighbors of neighbors will also be considered for the transformation of the pixel and so on. The specific results of this kernel size variation will depend entirely on the kernel in use, for example, in the case of blur kernels the image smoothing will be increased or in contour detection, the contours will be wider; it all depends on the kernel in use.\n","\n","- Explain the process of convolution and how it is used to apply a filter to an image.\n","   - Convolution requires two main components: the image to be filtered and the kernel to be applied to the image. First, the kernel must be flipped on both axes, in order to fulfill the mathematical background of the convolution; then we will have to go through all the pixels of the image subtracting the region of equivalent size to the kernel, once it has been centered on the current pixel; with this region and the kernel, the weighted sum is performed, placing in the output matrix the value obtained in the position equivalent to that of the image pixel used. The output matrix, at the end of the process on the whole image, will have been filtered by the given kernel.\n","\n","- In your own words, explain the significance of the Fourier transform in image processing.\n","\n","- Describe the process and the underlying principle of template matching using normalized cross-correlation.\n","   - In the NCC the process to follow will be similar to that of the convolution in the sense that we will go through each pixel of the image obtaining a new value, the difference is that the kernel flipping will not be performed (so we will follow more a correlation scheme as the name says) and the value will be obtained in a different way than the weighted sum. Obtaining values by NCC is done by identifying the quantitative squared difference, existing between a given template and a specific region of the same size as that template; therefore, if we want to find the most similar region, what we must do is to minimize this value. By the properties of the difference of squares, we will always have two constant terms and one variable, so we can focus only on the variable that is subtracting from the rest of the equation, so, if we only focus on this term to find the match between template and image region, given that the variable term subtracts from the rest of the equation, what we will seek will be to maximize its value. Finally, to keep all the pixel values of the image under the same range and to avoid that the match is benefited by increasing pixel intensities, we will perform the normalization of the values and this will be used as the divisor of the variable term. Thus obtaining the NCC formula which, at its maximum values, will give us the regions with the highest coincidence in the image according to a specific template.\n","\n","- Why might one choose to operate in the frequency domain (using Fourier transform) instead of the spatial domain when processing an image?\n","\n","- Can we remove \"salt and pepper\" noise with some of those linear filters?  \n","\n","\n","### Optional Deep Dive Questions:\n","\n","- Besides convolution, what are other methods to apply filters to an image? Can you compare and contrast these methods?\n","\n","- Explore the concept of separable kernels. What benefits do they provide in convolution?\n","\n","- What is the purpose of the padding in the convolution operation, and what are the different types of padding? Explain also the purpose of stride and how this affects the output?\n","\n","- Explain which types of filters might be important for a real-scenario application (e.g., self-driving car vision system, medical image analysis).\n","\n","- What are the limitations of linear filters when it comes to noise reduction? Are there scenarios where they might not be ideal?\n","\n","- How would might you extend the concepts of convolution to color images? Which complexities arise when dealing with multiple channels?\n","\n","- Fourier transform is used in various fields outside of image processing. Can you find and explain an application outside of computer vision?\n","\n","- Explore and explain the difference between the discrete Fourier transform (DFT) and the fast Fourier transform (FFT). Why might FFT be preferred in many applications?\n"]},{"cell_type":"markdown","metadata":{"id":"yFiyIjT_F3qF"},"source":["## Convolution operation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8DqpUP4vIf8j"},"source":["#### Objective:\n","Implement the convolution operation by hand and compare it with standard library functions. Also, manipulate image sizes through upsampling and downsampling using convolution. The focus will be on handling edge cases and experimenting with various kernels.\n","\n","---\n","\n","#### Guideline:\n","1. Implement a function to perform the convolution operation on an image. Make sure to handle edge cases and allow for different kernel sizes. DO NOT USE LIBRARY IMPLEMENTATIONS.\n","2. Develop methods for upsampling and downsampling images using your convolution function.\n","3. Experiment with your convolution function by applying different kernels on various images.\n","4. Compare your hand-implemented convolution outcomes with standard library methods such as OpenCV's `filter2d` and SciPy's `convolve`.\n","\n","---\n","\n","#### Expected results:\n","- Your hand-implemented convolution, upsampling, and downsampling code.\n","- Resized images at scales: (2x, 0.5x)."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698767081739,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"kSYWEDF0wMl9"},"outputs":[],"source":["# In this cell you will find the four functions requested\n","\n","def convolution(image, kernel, padding=\"SAME\", stride=1):\n","\n","    \"\"\"\n","    Providing an image that has been opened with OpenCV in IMREAD_GRAYSCALE mode works very well:\n","    image = cv2.imread(path_images,cv2.IMREAD_GRAYSCALE)\n","    \"\"\"\n","\n","    # Checking parameters\n","    if not(isinstance(image, np.ndarray) or image is None):\n","        raise ValueError(\"The 'image' parameter must be a NumPy matrix\")\n","    if not(len(image.shape) == 2):\n","      raise ValueError(\"The 'image' parameter must represent a black and white image (3D -> 2D)\")\n","    if not(isinstance(kernel, np.ndarray)):\n","        raise ValueError(\"The 'kernel' parameter must be a NumPy matrix\")\n","    if not(kernel.shape[0] == kernel.shape[1]):\n","        raise ValueError(\"The dimensions of the 'kernel' must be the same: {} != {}\".format(kernel.shape[0], kernel.shape[1]))\n","    #if kernel.shape[0] % 2 == 0 or kernel.shape[1] % 2 == 0:\n","    #    raise ValueError(\"The size of the 'kernel' must be odd\")\n","    if not(isinstance(stride, int) or stride <= 0):\n","        raise ValueError(\"The 'stride' parameter must be a strictly positive integer\")\n","    if not(padding == \"SAME\" or padding == \"VALID\"):\n","        raise ValueError(\"The 'padding' parameter must be 'SAME' or 'VALID', any other parameter is invalid.\")\n","\n","    # Converting padding to numbers\n","    if padding == \"SAME\": # Return an output image of the same size as the input image\n","      padding = (kernel.shape[0]-1)//2\n","    elif padding == \"VALID\": # Calculate only valid pixels, i.e. without padding\n","      padding = 0\n","    else:\n","      raise ValueError(\"The 'padding' parameter must be 'SAME' or 'VALID', any other parameter is invalid.\")\n","\n","    # Type verification\n","    image = image.astype(np.float64)\n","    kernel = kernel.astype(np.float64)\n","\n","    # Flipping the kernel\n","    kernel = np.flip(kernel, 0)\n","    kernel = np.flip(kernel, 1)\n","\n","    # Image and kernel dimensions\n","    image_height, image_width = image.shape\n","    kernel_height, kernel_width = kernel.shape\n","\n","    # Output size calculation\n","    out_height = m.floor(1 + ((image_height + 2 * padding - kernel_height) / stride))\n","    out_width = m.floor(1 + ((image_width + 2 * padding - kernel_width) / stride))\n","\n","    # Kernel half-size\n","    kh2, kw2 = kernel_height // 2, kernel_width // 2\n","\n","    # Initialization of the output matrix\n","    output = np.zeros((out_height, out_width))\n","    output = output.astype(np.float64)\n","\n","    # Padded image initialization\n","    pad_size = ((padding, padding), (padding, padding))\n","    padded_image = np.pad(image, pad_size, mode='constant', constant_values=0)\n","    padded_image = padded_image.astype(np.float64)\n","    padded_height, padded_width = padded_image.shape\n","\n","    # Applying convolution\n","    output_cell_i = 0\n","    output_cell_j = 0\n","    for i in range(kh2, padded_height - kh2, stride):\n","        output_cell_j = 0\n","        for j in range(kw2, padded_width - kw2, stride):\n","\n","            # Coordinates of the image region corresponding to the output\n","            start_i = i - kh2\n","            end_i = i + kh2\n","            start_j = j - kw2\n","            end_j = j + kw2\n","\n","            # Image region recovery\n","            image_region = padded_image[start_i:end_i + 1, start_j:end_j + 1]\n","\n","            # Calculating the scalar product between the image region and the kernel\n","            output[output_cell_i, output_cell_j] = np.sum(image_region * kernel)\n","\n","            # Incrementation\n","            output_cell_j += 1\n","        output_cell_i += 1\n","\n","    return output\n","\n","\n","def gaussian_kernel(size, sigma):\n","\n","    if size%2==0:\n","      raise ValueError(\"The kernel size must be an odd number.\")\n","\n","    kernel = np.fromfunction(\n","        lambda x, y: (1/(2*np.pi*sigma**2))*np.exp(-((x-(size//2))**2+(y-(size//2))**2)/(2*sigma**2)),\n","        (size, size)\n","    )\n","\n","    return(kernel/np.sum(kernel)) # Normalize the kernel so that the sum equals 1\n","\n","\n","def upscale_image(image, scale_factor):\n","\n","    # Parameter verification\n","    if not isinstance(scale_factor, int) or scale_factor <= 1:\n","        raise ValueError(\"The 'scale_factor' parameter must be an integer greater than 1.\")\n","\n","    # Get the dimensions of the input image\n","    height, width = image.shape\n","\n","    # Calculate the new dimensions\n","    new_height = height * scale_factor\n","    new_width = width * scale_factor\n","\n","    # Create a new image with upsampled dimensions\n","    upscaled_image = np.zeros((new_height, new_width))\n","    upscaled_image[::scale_factor, ::scale_factor] = image\n","\n","    # Prepare the convolution kernel\n","    kernel_size = 2 * scale_factor - 1\n","    middle = scale_factor - 1\n","    kernel = (1/4) * np.ones((kernel_size, kernel_size))\n","    kernel[:, middle] = 1/2\n","    kernel[middle, :] = 1/2\n","    kernel[middle, middle] = 1\n","\n","    # Convolution of the upsampled image\n","    upscaled_image = convolution(upscaled_image, kernel, padding=\"SAME\", stride=1)\n","\n","    return upscaled_image\n","\n","\n","def downscale_image(image, scale_factor):\n","\n","    # Parameter verification\n","    if not isinstance(scale_factor, int) or scale_factor <= 1:\n","        raise ValueError(\"The 'scale_factor' parameter must be an integer greater than 1.\")\n","\n","    # Prepare the convolution kernel for blurring\n","    kernel_size = 2 * scale_factor - 1\n","    kernel = (1/(kernel_size**2)) * np.ones((kernel_size, kernel_size))\n","\n","    # Convolution to downsampled the image\n","    downsampled_image = convolution(image,kernel,padding=\"SAME\",stride=scale_factor)\n","\n","    return downsampled_image\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698767081739,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"skyxMJbAnY48"},"outputs":[],"source":["# In this cell, you will find four functions to test our own convolution function in the next cell.\n","\n","def conv2d_tf(image, kernel, stride, padding):\n","  # Image processing\n","  image = image.astype(np.float64)\n","  image = tf.constant(image, dtype=tf.float64)\n","  image = tf.expand_dims(image, axis=0) # Add dimensions for batch\n","  image = tf.expand_dims(image, axis=-1) # Add dimensions for channel\n","  # Kernel processing\n","  kernel = kernel.astype(np.float64)\n","  kernel = np.flip(kernel, 0)\n","  kernel = np.flip(kernel, 1)\n","  kernel = tf.constant(kernel, dtype=tf.float64)\n","  kernel = tf.expand_dims(kernel, axis=-1) # Add dimensions for batch\n","  kernel = tf.expand_dims(kernel, axis=-1) # Add dimensions for channel\n","  # Convolution\n","  #output = tf.nn.conv2d(image, kernel, strides=[1, stride, stride, 1], padding=padding)\n","  # I manually apply stride at the end since it doesn't take the same value as mine\n","  # Without this, there is an arbitrary 1-pixel offset\n","  output = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding=padding)\n","  # Output processing\n","  output = output.numpy()\n","  output = output[0, :, :, 0]\n","  output = output[::stride, ::stride]\n","  return output\n","\n","def convolution_function_testing_scipy(image_original, iteration_number=20, coefficient_value_range=10, coefficients_type=\"random\", kernel_size_range=9, precision=6):\n","  # Iterating over the number of tests\n","  for i in range(iteration_number):\n","    print(\"\")\n","    print(\"--> Scipy Iteration \", i + 1)\n","    # Generating parameters\n","    kernel_size = random.randint(3, kernel_size_range)\n","    # Ensuring the kernel size is odd\n","    #kernel_size = kernel_size if kernel_size % 2 != 0 else kernel_size + 1\n","    kernel_size = kernel_size if kernel_size % 2 == 0 else kernel_size + 1\n","    # Generating a random kernel\n","    kernel = np.empty((kernel_size, kernel_size))\n","    if coefficients_type == \"random\":\n","      coefficients_type_ = random.choice([\"int\", \"float\"])\n","    else:\n","      coefficients_type_ = coefficients_type\n","    if coefficients_type_ == \"int\":\n","      kernel = np.random.randint(-coefficient_value_range, coefficient_value_range, size=(kernel_size, kernel_size))\n","    elif coefficients_type_ == \"float\":\n","      kernel = np.round(np.random.uniform(-coefficient_value_range, coefficient_value_range, size=(kernel_size, kernel_size)), precision)\n","    # Displaying the kernel for the user\n","    print(\"kernel = \\n\", kernel)\n","    # Custom convolutions (to be verified)\n","    image_convolution_perso_valid = convolution(image_original, kernel, padding=\"VALID\", stride=1)\n","    image_convolution_perso_same = convolution(image_original, kernel, padding=\"SAME\", stride=1)\n","    # Scipy convolutions (already functional for verification)\n","    image_convolution_scipy_valid = convolve2d(image_original, kernel, mode='valid')\n","    image_convolution_scipy_same = convolve2d(image_original, kernel, mode='same')\n","    # Validity test of the \"valid\" results\n","    if np.array_equal(np.round(image_convolution_perso_valid, precision), np.round(image_convolution_scipy_valid, precision)):\n","      print(\"The matrices image_convolution_perso_valid and image_convolution_scipy_valid are equal.\")\n","    else:\n","      print(\"The matrices image_convolution_perso_valid and image_convolution_scipy_valid are not equal.\")\n","      print(image_convolution_perso_valid)\n","      print(image_convolution_scipy_valid)\n","    # Validity test of the \"same\" results\n","    if np.array_equal(np.round(image_convolution_perso_same, precision), np.round(image_convolution_scipy_same, precision)):\n","      print(\"The matrices image_convolution_perso_same and image_convolution_scipy_same are equal.\")\n","    else:\n","      print(\"The matrices image_convolution_perso_same and image_convolution_scipy_same are not equal.\")\n","      print(image_convolution_perso_same)\n","      print(image_convolution_scipy_same)\n","  print(\" \")\n","\n","def convolution_function_testing_tensorflow(image_original, iteration_number=20, coefficient_value_range=10, coefficients_type=\"random\", stride_range=5, kernel_size_range=9, precision=6):\n","  # Iterating over the number of tests\n","  for i in range(iteration_number):\n","    print(\"\")\n","    print(\"--> TensorFlow Iteration \", i + 1)\n","    # Generating parameters\n","    stride = random.randint(1, stride_range)\n","    kernel_size = random.randint(3, kernel_size_range)\n","    # Ensuring the kernel size is odd\n","    kernel_size = kernel_size if kernel_size % 2 != 0 else kernel_size + 1\n","    # Generating a random kernel\n","    kernel = np.empty((kernel_size, kernel_size))\n","    if coefficients_type == \"random\":\n","      coefficients_type_ = random.choice([\"int\", \"float\"])\n","    else:\n","      coefficients_type_ = coefficients_type\n","    if coefficients_type_ == \"int\":\n","      kernel = np.random.randint(-coefficient_value_range, coefficient_value_range, size=(kernel_size, kernel_size))\n","    elif coefficients_type_ == \"float\":\n","      kernel = np.round(np.random.uniform(-coefficient_value_range, coefficient_value_range, size=(kernel_size, kernel_size)), precision)\n","    # Displaying the kernel for the user\n","    print(\"kernel = \\n\", kernel)\n","    # Custom convolutions (to be verified)\n","    image_convolution_perso_valid = convolution(image_original, kernel, padding=\"VALID\", stride=stride)\n","    image_convolution_perso_same = convolution(image_original, kernel, padding=\"SAME\", stride=stride)\n","    # TensorFlow convolutions (already functional for verification)\n","    image_convolution_tf_valid = conv2d_tf(image_original, kernel, stride=stride, padding=\"VALID\")\n","    image_convolution_tf_same = conv2d_tf(image_original, kernel, stride=stride, padding=\"SAME\")\n","    # Validity test of the \"valid\" results\n","    if np.array_equal(np.round(image_convolution_perso_valid, precision), np.round(image_convolution_tf_valid, precision)):\n","      print(\"The matrices image_convolution_perso_valid and image_convolution_tf_valid are equal.\")\n","    else:\n","      print(\"The matrices image_convolution_perso_valid and image_convolution_tf_valid are not equal.\")\n","      print(image_convolution_perso_valid)\n","      print(image_convolution_tf_valid)\n","    # Validity test of the \"same\" results\n","    if np.array_equal(np.round(image_convolution_perso_same, precision), np.round(image_convolution_tf_same, precision)):\n","      print(\"The matrices image_convolution_perso_same and image_convolution_tf_same are equal.\")\n","    else:\n","      print(\"The matrices image_convolution_perso_same and image_convolution_tf_same are not equal.\")\n","      print(image_convolution_perso_same)\n","      print(image_convolution_tf_same)\n","  print(\" \")\n","\n","def display_separation(text):\n","  print(\"\")\n","  print(\"-------------------------------------------------------------------\")\n","  print(\"\")\n","  print(\"----> \",text)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"executionInfo":{"elapsed":1536,"status":"error","timestamp":1698767083273,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"tSf4uej4BfrG","outputId":"68cfc7ba-f3b2-4c57-c3e4-239242025d50"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","-------------------------------------------------------------------\n","\n","---->  Convolution function test\n","\n","--> Scipy Iteration  1\n","kernel = \n"," [[-4.305797  6.840923 -8.215657  1.889547 -1.660284 -3.283383]\n"," [-8.843722  1.333068 -1.203378  6.463982 -9.764868 -3.101379]\n"," [ 6.728576  1.124639 -4.619349 -2.93473  -3.899936  0.566305]\n"," [ 5.196434  1.101152  8.713026  0.702625  6.959287 -8.22346 ]\n"," [-4.044336 -0.389471  9.783784 -8.343699  1.624674  8.392083]\n"," [-7.688291  1.630959 -3.126604  8.64425  -3.957041  6.622125]]\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-10ca19d02986>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdisplay_separation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Convolution function test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mconvolution_function_testing_scipy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mconvolution_function_testing_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-590083433ca0>\u001b[0m in \u001b[0;36mconvolution_function_testing_scipy\u001b[0;34m(image_original, iteration_number, coefficient_value_range, coefficients_type, kernel_size_range, precision)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kernel = \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Custom convolutions (to be verified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mimage_convolution_perso_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VALID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mimage_convolution_perso_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Scipy convolutions (already functional for verification)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-f4f1d8f4c29a>\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(image, kernel, padding, stride)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The 'image' parameter must be a NumPy matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The 'image' parameter must represent a black and white image (3D -> 2D)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}],"source":["# In this cell, you will find the tests to prove that the four requested functions work correctly\n","\n","# Image reading\n","\n","path = os.path.join(path_images, \"cameraman.png\")\n","image_original = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","\n","# Test of the convolution function\n","\n","display_separation(\"Convolution function test\")\n","convolution_function_testing_scipy(image_original, iteration_number=1)\n","convolution_function_testing_tensorflow(image_original, iteration_number=1)\n","\n","# Test of the gaussian_kernel function\n","\n","display_separation(\"Gaussian Kernel function test\")\n","for size in range(3, 6, 2):\n","    for sigma in range(1, 2):\n","        kernel = gaussian_kernel(size, sigma)\n","        print(\"---\")\n","        print(\"Size = {}, Sigma = {}, Kernel = \".format(size, sigma))\n","        print(kernel)\n","\n","# Test of the upscale_image function\n","\n","display_separation(\"Upscale Image function test\")\n","scale_factors = [2, 3, 4]\n","fig, axes = plt.subplots(1, len(scale_factors) + 1, figsize=(4 * (len(scale_factors) + 1), 4))\n","axes[0].imshow(image_original, cmap='gray')\n","axes[0].set_title(f\"Original Image, Shape {image_original.shape}\")\n","for i, scale_factor in enumerate(scale_factors):\n","    print(f\"Upscaling in progress for scale_factor {scale_factor}\")\n","    upscaled_image = upscale_image(image_original, scale_factor)\n","    axes[i + 1].imshow(upscaled_image, cmap='gray')\n","    axes[i + 1].set_title(f\"Factor {scale_factor}, Shape {upscaled_image.shape}\")\n","plt.show()\n","\n","# Test of the downscale_image function\n","\n","display_separation(\"Downscale Image function test\")\n","scale_factors = [2, 3, 4, 5]\n","fig, axes = plt.subplots(1, len(scale_factors) + 1, figsize=(4 * (len(scale_factors) + 1), 4))\n","axes[0].imshow(image_original, cmap='gray')\n","axes[0].set_title(f\"Original Image, Shape {image_original.shape}\")\n","for i, scale_factor in enumerate(scale_factors):\n","    print(f\"Downscaling in progress for scale_factor {scale_factor}\")\n","    downscaled_image = downscale_image(image_original, scale_factor)\n","    axes[i + 1].imshow(downscaled_image, cmap='gray')\n","    axes[i + 1].set_title(f\"Factor {scale_factor}, Shape {downscaled_image.shape}\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"bIfr3ZZytPti"},"source":["## Linear Filtering\n"]},{"cell_type":"markdown","metadata":{"id":"rO9jAcmwHFgi"},"source":["#### Objective:\n","Explore various image filtering techniques by applying them to a set of example images as well as images of your own choice. Experiment with different kernel sizes and types of filters to observe their effects.\n","\n","- Blur:\n","  - Average Box Filter\n","  - Gaussian Filter\n","- Edge:\n","  - Laplacian Filter\n","  - Sobel Filter\n","  - Prewitt Filter\n","\n","Feel free to experiment with additional filters as well.\n","\n","---\n","\n","#### Guideline:\n","1. Apply each of the given filters to the example images provided.\n","2. Use images of your own choosing to further experiment with these filters.\n","3. Experiment with varying kernel sizes for each filter.\n","4. Analyze the effects of each filter and kernel size on the images.\n","\n","---\n","\n","#### Expected results:\n","- Filtered images (both example and your own)\n","- Analysis of the effect of kernel size\n"]},{"cell_type":"markdown","metadata":{"id":"LH36ulMF0fD6"},"source":["Examples of different linear filters for blur and edge *detection*\n","\n","1. **Average Box Filter:**\n","   ```\n","   1/9  1/9  1/9\n","   1/9  1/9  1/9\n","   1/9  1/9  1/9\n","   ```\n","\n","2. **Gaussian Blur Filter:**\n","   ```\n","   1/16  2/16  1/16\n","   2/16  4/16  2/16\n","   1/16  2/16  1/16\n","   ```\n","\n","3. **Laplacian Filter:**\n","   ```\n","   0  1  0\n","   1 -4  1\n","   0  1  0\n","   ```\n","\n","4. **Sobel Operator (horizontal || vertical):**\n","   ```\n","     -1  0  1  ||  -1 -2 -1\n","     -2  0  2  ||   0  0  0\n","     -1  0  1  ||   1  2  1\n","  ```\n","\n","5. **Prewitt Operator (horizontal || vertical):**\n","   ```\n","    -1  0  1  || -1 -1 -1\n","    -1  0  1  ||  0  0  0\n","    -1  0  1  ||  1  1  1\n","   ```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698767083273,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"-0TvzdBZ775h"},"outputs":[],"source":["# Kernels definitions\n","\n","kernel_averageBox = np.array([[1/9,1/9,1/9],\n","                              [1/9,1/9,1/9],\n","                              [1/9,1/9,1/9]])\n","\n","kernel_gaussianBlur = np.array([[1/16,2/16,1/16],\n","                                [2/16,4/16,2/16],\n","                                [1/16,2/16,1/16]])\n","\n","kernel_laplacian = np.array([[0,1,0],\n","                             [1,-4,1],\n","                             [0,1,0]])\n","\n","kernel_sobelOperatorHorizontal = np.array([[-1,0,1],\n","                                           [-2,0,2],\n","                                           [-1,0,1]])\n","\n","kernel_sobelOperatorHorizontal_5x5 = np.array([[1, 2, 0, -2, -1],\n","[4, 8, 0, -8, -4],\n","[6, 12, 0, -12, -6],\n","[4, 8, 0, -8, -4],\n","[1, 2, 0, -2, -1]])\n","\n","kernel_sobelOperatorVertical = np.array([[-1,-2,-1],\n","                                         [0,0,0],\n","                                         [1,2,1]])\n","\n","kernel_prewittOperatorHorizontal = np.array([[-1,0,1],\n","                                             [-1,0,1],\n","                                             [-1,0,1]])\n","\n","kernel_prewittOperatorVertical = np.array([[-1,-1,-1],\n","                                           [0,0,0],\n","                                           [1,1,1]])\n","\n","kernel_prewittOperatorVertical_7x7 = np.array([\n","    [-4, -4, -4, -4, -4, -4, -4],\n","    [-2, -2, -2, -2, -2, -2, -2],\n","    [-1, -1, -1, -1, -1, -1, -1],\n","    [0, 0, 0, 0, 0, 0, 0],\n","    [1, 1, 1, 1, 1, 1, 1],\n","    [2, 2, 2, 2, 2, 2, 2],\n","    [4, 4, 4, 4, 4, 4, 4]\n","])\n","\n","kernel_averageBox_5x5 = np.array([[1/25,1/25,1/25,1/25,1/25],\n","                                    [1/25,1/25,1/25,1/25,1/25],\n","                                    [1/25,1/25,1/25,1/25,1/25],\n","                                    [1/25,1/25,1/25,1/25,1/25],\n","                                    [1/25,1/25,1/25,1/25,1/25]])\n","\n","kernel_gaussianBlur_5x5 = np.array([\n","    [1/256, 4/256, 6/256, 4/256, 1/256],\n","    [4/256, 16/256, 24/256, 16/256, 4/256],\n","    [6/256, 24/256, 36/256, 24/256, 6/256],\n","    [4/256, 16/256, 24/256, 16/256, 4/256],\n","    [1/256, 4/256, 6/256, 4/256, 1/256]\n","])\n","\n","kernel_laplacian_5x5 = np.array([[0,0,1,0,0],\n","                                  [0,1,-1,1,0],\n","                                  [1,-1,-4,-1,1],\n","                                  [0,1,-1,1,0],\n","                                  [0,0,1,0,0]])\n","\n","kernel_gaussianBlur_7x7 = np.array([\n","    [1/140, 1/70, 2/140, 2/140, 2/140, 1/70, 1/140],\n","    [1/70, 2/70, 2/70, 4/140, 2/70, 2/70, 1/70],\n","    [2/140, 2/70, 4/70, 8/140, 4/70, 2/70, 2/140],\n","    [2/140, 4/140, 8/140, 16/140, 8/140, 4/140, 2/140],\n","    [2/140, 2/70, 4/70, 8/140, 4/70, 2/70, 2/140],\n","    [1/70, 2/70, 2/70, 4/140, 2/70, 2/70, 1/70],\n","    [1/140, 1/70, 2/140, 2/140, 2/140, 1/70, 1/140]\n","])\n","\n","kernel_test = np.array([[1,0,0,0,1],\n","                        [0,0,0,0,0],\n","                        [0,0,-1,0,0],\n","                        [0,0,0,0,0],\n","                        [1,0,0,0,1]])\n","\n","kernels = [kernel_averageBox,\n","           kernel_averageBox_5x5,\n","           kernel_gaussianBlur,\n","           kernel_gaussianBlur_5x5,\n","           kernel_gaussianBlur_7x7,\n","           kernel_laplacian,\n","           kernel_laplacian_5x5,\n","           kernel_sobelOperatorHorizontal,\n","           kernel_sobelOperatorHorizontal_5x5,\n","           kernel_sobelOperatorVertical,\n","           kernel_prewittOperatorHorizontal,\n","           kernel_prewittOperatorVertical,\n","           kernel_prewittOperatorVertical_7x7]\n","\n","kernels_name = [\"averageBox\",\n","                \"averageBox_5x5\",\n","                \"gaussianBlur\",\n","                \"gaussianBlur_5x5\",\n","                \"kernel_gaussianBlur_7x7\",\n","                \"laplacian\",\n","                \"laplacianBigger\",\n","                \"sobelOperatorHorizontal\",\n","                \"sobelOperatorHorizontal_5x5\",\n","                \"sobelOperatorVertical\",\n","                \"previttOperatorHorizontal\",\n","                \"prewittOperatorVertical\",\n","                \"kernel_prewittOperatorVertical_7x7\"]\n","\n","# List of images urls\n","\n","\"\"\"\n","urls = [\"./images/distorted_checkers.jpg\",\n","        \"./images/cameraman.png\",\n","        \"./images/lenna.png\",\n","        \"./images/lalo.jpg\",\n","        \"./images/Simi.jpg\",\n","        \"./images/amlo.jpg\",\n","        \"./images/tianguis greyscale.jpg\"] # It doesn't work in google colab because the file management is different.\n","\"\"\"\n","\n","urls = [os.path.join(path_images, \"distorted_checkers.jpg\"),\n","        os.path.join(path_images, \"cameraman.png\"),\n","        os.path.join(path_images, \"lenna.png\"),\n","        os.path.join(path_images, \"lalo.jpg\"),\n","        os.path.join(path_images, \"Simi.jpg\"),\n","        os.path.join(path_images, \"amlo.jpg\"),\n","        os.path.join(path_images, \"tianguis greyscale.jpg\")]\n","\n","images_name = [\"distorted_checkers.jpg\",\n","                \"cameraman.png\",\n","                \"lenna.png\",\n","                \"lalo.jpg\",\n","                \"Simi.jpg\",\n","                \"amlo.jpg\",\n","                \"tianguis greyscale.jpg\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698767083274,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"Ks7qa4CfGC4U"},"outputs":[],"source":["# Apply linear filter with a desired image and kernel\n","def apply_linear_filter(image, kernel):\n","  filtered_image_array = convolution(image, kernel)\n","  return(Image.fromarray(filtered_image_array))\n","\n","# Converts a list of urls into a list of images\n","def images_list(urls):\n","  images = []\n","  for url in urls:\n","    images.append(cv2.imread(url, cv2.IMREAD_GRAYSCALE))\n","  return(images)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698767083274,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"elYYajfA775h"},"outputs":[],"source":["# Creating list of images\n","images = images_list(urls)\n","\n","# Plotting images\n","fig, axs = plt.subplots(len(images), len(kernels), figsize=(4 * (len(kernels)), 4 * (len(images))), sharex=True, sharey=True)\n","fig.suptitle(\"Linearly filtered images\")\n","print(\"--> Image processing\")\n","for i, image,image_name in zip(range(len(images)), images, images_name):\n","    print(\"Image processing\",image_name)\n","    # Resize image for faster execution\n","    # We arbitrarily choose to resize the images to around 150² pixels, as execution is relatively fast.\n","    scale_factor = round(m.sqrt((image.shape[0]*image.shape[1])/(150**2)))\n","    if scale_factor>1:\n","      image = downscale_image(image,scale_factor)\n","    for j, kernel, kernel_name in zip(range(len(kernels)), kernels, kernels_name):\n","        transformed_image = apply_linear_filter(image, kernel)\n","        axs[i,j].axis(True)\n","        axs[i,j].imshow(transformed_image)\n","        if i == 0:\n","          axs[i,j].set_title(f\"{kernel_name}\")\n","print(\"--> Display preparation\")\n","fig.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"-lPrIBV4NoJ_"},"source":["##### Kernel Analysis\n","*In the case of the blur filters, the change is noticeable as the kernel size increases, with the 3x3 kernel diluting the image the least, and as the dimensions increase, the image becomes increasingly difficult to perceive.*\n","\n","*In comparison, the Average Box filter shows a much stronger attenuation compared to Gaussian Blur, so more detail is lost. However, Gaussian blur appears to enhance the intensity of colors by increasing the contrast of dark areas versus light areas, a phenomenon that is not present in the Average Box filter.*\n","\n","*In the case of the contour detection filters, it is necessary to mention that the increase in size in the dimensions of the kernel used simply widens the strokes that are already perfectly perceptible in the 3x3 versions of each kernel, becoming detrimental when the image has many strokes and the width of the contours found is such that the correct segmentation of the image is lost, as happened with the 5x5 horizontal Sobel Operator or the 7x7 vertical Prewitt Operator.*\n","\n","*About each filter we can say that the Laplacian kernel is very effective in detecting the contours of the main components of the image, perfectly attenuating backgrounds and textures. In the case of the directional filters we can notice a great similarity in the results of the vertical and horizontal versions of the Sobel and Prewitt operator respectively, each doing a great job in the directional detection of contours, although, usually, the Sobel Operator highlighted more intensely those contours.*"]},{"cell_type":"markdown","metadata":{"id":"5YdiK7BU-M35"},"source":["## Template matching\n"]},{"cell_type":"markdown","metadata":{"id":"bwuuHUNDG7Mr"},"source":["#### **Objective:**\n","Implement template matching system using normalized cross-correlations (NCC). You need to identify multiple templates within a given image and draw bounding boxes around the identified regions.\n","\n","---\n","\n","#### Guidelines:\n","1. Implement the normalized cross-correlation algorithm. Your function should accept an image and a template as parameters. DO NOT USE LIBRARIES.\n","3. Use a folder containing multiple templates to attempt to identify all templates within a given test image.\n","4. Draw bounding boxes around the identified template regions in the test image.\n","---\n","\n","#### Expected result:\n","- A test image with bounding boxes around identified templates"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698767083274,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"QzIGRTqYPGrs"},"outputs":[],"source":["# Definition of urls\n","templates_url = [os.path.join(path_images, \"template/template_Circle.png\"),\n","                 os.path.join(path_images, \"template/template_E.png\"),\n","                 os.path.join(path_images, \"template/template_H.png\"),\n","                 os.path.join(path_images, \"template/template_L.png\"),\n","                 os.path.join(path_images, \"template/template_O.png\"),\n","                 os.path.join(path_images, \"template/template_Square.png\"),\n","                 os.path.join(path_images, \"template/template_Triangle.png\"),\n","                 os.path.join(path_images, \"template/template_Circle1.png\"),\n","                 os.path.join(path_images, \"template/template_L1.png\"),\n","                 os.path.join(path_images, \"template/template_Square1.png\")]\n","\n","base_image_url = os.path.join(path_images, \"template/template_example.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698767083274,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"zlM4SEvoDfWx"},"outputs":[],"source":["# Definition of functions\n","\n","# NCC function\n","def NCC_template_matching(image, template):\n","    # Getting dimensions\n","    m_image, n_image = image.shape\n","    m_template, n_template = template.shape\n","\n","    # NCC array for values storage\n","    ncc_map = np.zeros_like(image)\n","\n","    # Mean value for template\n","    template_mean = np.mean(template)\n","\n","    # Variables to keep track of max values spots\n","    max_cordinates = []\n","    max = -np.inf  # Initialize to negative infinity to ensure any NCC will be greater\n","\n","    # Obtaining NCC values for each fittable template patch at image\n","    i = 0\n","    while i + m_template <= m_image:\n","        j = 0\n","        while j + n_template <= n_image:\n","            image_region = image[i : i + m_template, j : j + n_template] # Extracting template size patch from image\n","\n","            image_region_mean = np.mean(image_region) # Mean of image patch\n","\n","            # NCC value calculation\n","            num = np.sum((image_region - image_region_mean) * (template - template_mean))\n","            den = np.std(image_region) * np.std(template)\n","            if den == 0:\n","              ncc = 0\n","            else:\n","              ncc = num / den\n","            ncc_map[i, j] = ncc\n","\n","            # Storing max values and its coordinates\n","            if ncc == max:\n","                max_cordinates.append((i, j))\n","            elif ncc > max:\n","                max = ncc\n","                max_cordinates = [] # we reset the list\n","                max_cordinates.append((i, j))\n","            j += 1\n","        i += 1\n","\n","    return ncc_map, max_cordinates\n","\n","# Draw matching greatest match patch boundaries\n","def draw_boundaries(image, kernel, coordinates):\n","    m_kernel, n_kernel = kernel.shape # Sizes of kernel for boundarie size\n","\n","    pil_image = Image.fromarray(image) # Convert np array to pil image for drawing of boundaries\n","\n","    # Boundarie (rectangle) draw\n","    draw_image = ImageDraw.Draw(pil_image)\n","    shape = [(coordinates[1], coordinates[0]), (coordinates[1] + n_kernel, coordinates[0] + m_kernel)]\n","    draw_image.rectangle(shape, outline = 'red')\n","\n","    image = np.array(pil_image) # Return Pil image to np array\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698767083274,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"dv2R4ATbDfWx"},"outputs":[],"source":["# transforming urls to images\n","templates_images = images_list(templates_url)\n","\n","# Obtaining image and a copy to draw boundaries\n","base_image = cv2.imread(base_image_url, cv2.IMREAD_GRAYSCALE)\n","bounded_image = base_image\n","\n","fig, axs = plt.subplots(len(templates_images), 2, figsize=(12, 2*(len(templates_images))),sharey=True, sharex=True) # Plot for boundaries for each template\n","fig.suptitle(\"Greatest match finded by template\")\n","\n","for i, template_image in zip(range(len(templates_images)), templates_images):\n","    output, max_cordinates = NCC_template_matching(base_image, template_image) # Obtaining NCC values for each template\n","\n","    axs[i,0].axis('auto') # Ploting template\n","    axs[i,0].imshow(template_image)\n","\n","    template_bounded_image = base_image # Creating a copy for each template bounded image\n","\n","    for cordinate in max_cordinates:\n","        template_bounded_image = draw_boundaries(template_bounded_image, template_image, cordinate) # Draw boundaries for single template plot\n","        bounded_image = draw_boundaries(bounded_image, template_image, cordinate) # Draw boundaries at final output plot\n","\n","    # Plotting single template boundaries\n","    axs[i,1].axis('auto')\n","    axs[i,1].imshow(template_bounded_image)\n","\n","fig.tight_layout()\n","plt.show()\n","\n","# Plotting final output plot\n","fig, axs = plt.subplots(2, 1, sharex=True, sharey=True)\n","fig.suptitle(\"NCC Pattern Matching\")\n","axs[0].imshow(base_image)\n","axs[1].imshow(bounded_image)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DjquusMuLAP-"},"source":["## Fourier Transform\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xQ438ICeG2LM"},"source":["#### **Objective**:\n","You are provided with three images, each corrupted by one or more periodic frequencies. Your task is to process these images using Fourier Transform techniques to identify and remove the corrupting frequencies.\n","\n","- Image 1 has one specific corrupting frequency.\n","- Image 2 has another unique corrupting frequency.\n","- Image 3 has both frequencies from Image 1 and Image 2.\n","\n","---\n","\n","#### Guideline:\n","1. Use Fourier Transform to convert each image to the frequency domain. (you can use libraries for this)\n","2. Analyze the Fourier spectrum to identify the corrupting frequencies.\n","3. Remove the identified frequencies and perform an Inverse Fourier Transform to obtain the cleaned image.\n","4. Compare the cleaned images with the original corrupted ones and quantify the improvements.\n","\n","---\n","#### Expected results:\n","- Cleaned images\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698767083274,"user":{"displayName":"Maxence MARTIN","userId":"05470086628208192496"},"user_tz":-60},"id":"q4lo__1F9lmH"},"outputs":[],"source":["def average_neighbor(matrix, x, y):\n","    neighbors = [(x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1),\n","                (x - 1, y - 1), (x - 1, y + 1), (x + 1, y - 1), (x + 1, y + 1)]\n","    neighbor_sum = 0\n","    for neighbor_x, neighbor_y in neighbors:\n","        if 0 <= neighbor_x < matrix.shape[0] and 0 <= neighbor_y < matrix.shape[1]:\n","            neighbor_sum += matrix[neighbor_x, neighbor_y]\n","    average = neighbor_sum / len(neighbors)\n","    return average\n","\n","def display_module(module):\n","    fig, ax = plt.subplots()\n","    im = ax.contourf(module, cmap='viridis')\n","    plt.colorbar(im)\n","    plt.show()\n","\n","def filtering(image_name):\n","\n","  print(\"---------------------------------------------------------------------------------\")\n","  print(\"\\n\",\"Image :\",os.path.basename(image_name),\"\\n\")\n","\n","  # Original Image\n","  image_original = cv2.imread(image_name)\n","  image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2GRAY)\n","  image_original = image_original.astype(np.uint8)\n","\n","  # Fourier Transform\n","  fft_image = np.fft.fft2(image_original)  # FFT -> complex frequency\n","  fft_image = np.fft.fftshift(fft_image)  # Center the Fourier transform\n","\n","  # Module and Phase\n","  module = np.abs(fft_image)\n","  phase = np.angle(fft_image)\n","\n","  # Display module\n","  print(\"--> Module before cleaning the corrumpus frequencies.\")\n","  display_module(module)\n","\n","  # Filtering\n","  threshold = 4000000  # Threshold determined by observation of module outliers\n","  indices = np.argwhere(module > threshold)  # List of indices of elements exceeding the threshold\n","  print(f\"--> There are {len(indices)-1} points on the module that are abnormally high. These are corrumpus frequencies that need to be cleaned.\")\n","  for index in indices:\n","    x, y = index[0], index[1]\n","    if not (x == image_original.shape[0] // 2 and y == image_original.shape[1] // 2):\n","      # Only the point (x, y) = (image_original.shape[0] // 2, image_original.shape[1] // 2) is irrelevant\n","      # It is the central peak or zero frequency peak\n","      # For all other retrieved points, assign them the average value of their neighbors\n","      module[x, y] = average_neighbor(module, x, y)\n","\n","  # Display module after filtering\n","  print(\"--> Module after cleaning the corrumpus frequencies.\")\n","  display_module(module)\n","\n","  # Reconstruction of fft_image after filtering\n","  real_part = module * np.cos(phase)\n","  imag_part = module * np.sin(phase)\n","  complex_image = real_part + 1j * imag_part  # complex_image = filtering(fft_image)\n","\n","  # Inverse Fourier Transform to reconstruct the original image after filtering\n","  reconstruction_after_filtering = np.fft.ifftshift(complex_image) # Inverse FFT shift\n","  #reconstruction_after_filtering = np.abs(np.fft.ifft2(reconstruction_after_filtering)) # Inverse FFT\n","  reconstruction_after_filtering = np.fft.ifft2(reconstruction_after_filtering).real # Inverse FFT\n","  reconstruction_after_filtering = reconstruction_after_filtering.astype(np.uint8)  # Proper encoding\n","  reconstruction_after_filtering = np.clip(reconstruction_after_filtering, 0, 255)  # Values in [0, 255]\n","\n","  # Displaying filter results\n","  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","  axes[0].imshow(image_original, cmap='gray')\n","  axes[1].imshow(reconstruction_after_filtering, cmap='gray')\n","  plt.show()\n","\n","  # Quantifying image enhancement through filtering via calculation of PSNR and SSIM metrics\n","  # Original uncorrupted image\n","  uncorrupted_image = cv2.imread(os.path.join(path_images, \"lenna.png\"))\n","  uncorrupted_image = cv2.cvtColor(uncorrupted_image, cv2.COLOR_BGR2GRAY)\n","  uncorrupted_image = uncorrupted_image.astype(np.uint8)\n","  # Calculation\n","  psnr_image = psnr(uncorrupted_image,reconstruction_after_filtering)\n","  ssim_image = ssim(uncorrupted_image,reconstruction_after_filtering)\n","  # Display\n","  print(\"--> Quantifying image enhancement through filtering via calculation of PSNR and SSIM metrics\")\n","  print(f\"PSNR: {round(psnr_image,2)} dB, PSNR %: {round((psnr_image/56.00450571076812)*100,2)} %, SSIM: {round(ssim_image,2)}\")\n","  print(\"--> How to understand these metrics?\")\n","  print(\"PSNR: The higher the PSNR, the better the quality of the cleaned image.\")\n","  print(\"PSNR %: This is a personal metric that I added which allows you to reduce the PSNR as a percentage compared to its maximum value on this image. So the closer the PSNR% is to 100, the better the cleaning.\")\n","  print(\"SSIM: SSIM varies between -1 and 1, the closer it is to 1 the better the cleaning.\")\n","\n","\n","images = [\"fourier_1.jpg\", \"fourier_2.jpg\", \"fourier_3.jpg\"]\n","filtering(os.path.join(path_images, \"lenna.png\"))\n","for image in images:\n","  filtering(os.path.join(path_images, \"fourier\", image))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}